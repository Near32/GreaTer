# Weights & Biases Sweep Configuration for SDLM SmolLM2-135M Optimization
# Based on transfer_sdlm_smollm2_135m_instruct_gpu configuration for GSM8K dataset
project: GreaTer-SDLM-GSM8k 
entity: kevin-near32-org
program: main.py
command:
  - ${env}
  - ./venv/bin/python
  - ${program}
  - ${args}
method: bayes  # Bayesian optimization for efficient hyperparameter exploration
metric:
  #name: train/instantaneous_loss 
  name: train/loss 
  goal: minimize

# Early termination configuration for efficient resource utilization
#early_terminate:
#  type: hyperband
#  min_iter: 15
#  eta: 3
#  s: 2

# Fixed parameters (derived from SmolLM2-135M bash script configuration)
parameters:
  # Core configuration and model setup
  config:
    value: "./configs/transfer_sdlm_smollm2_135m_instruct_gpu.py"
  
  # Weights & Biases integration
  config.use_wandb:
    value: True
  config.project:
    value: "GreaTer-SDLM"
  
  # Data configuration
  config.train_data:
    value: "../data/grade_school_math/train.jsonl"
  config.test_data:
    value: "../data/grade_school_math/test.jsonl"
  config.result_prefix:
    value: "results/sdlm_smollm2_135m_gpu_gsm8k"
  
  # Training behavior and constraints
  config.stop_on_success:
    value: True
  config.allow_non_ascii:
    value: False
  config.num_train_models:
    value: 1
  config.n_train_data:
    value: 100
  config.n_test_data:
    value: 10
  config.anneal:
    value: true
  
  # Model-specific SDLM parameters
  config.sdlm_variable_kwargs.init_strategy:
    values: ['fluency', 'random']
  config.sdlm_variable_kwargs.learning_rate:
    values: [0.1, 0.01, 0.001]
  config.sdlm_variable_kwargs.temperature:
    values: [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
  config.sdlm_variable_kwargs.learnable_temperature:
    values: [False, True]
  config.sdlm_variable_kwargs.logit_scaler:
    values: [1.0, 10.0, 100.0]
  config.temp:
    values: [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
  
  config.sdlm_model_kwargs.temperature:
    values: [0.1, 0.5, 0.7, 1.0]
  config.sdlm_model_kwargs.learnable_temperature:
    values: [False, True]
  config.sdlm_model_kwargs.hidden_state_conditioning:
    values: [False, True]
  config.acc_grad_n_examples:
    values: [2, 4, 8, 16]
  config.gradient_comp_batch_size:
    values: [1, 2]
  config.update_solution_max_new_tokens:
    value: 256
  config.max_new_tokens_answer:
    value: 8
  
  # Prompt engineering configuration
  config.control_init:
    value: "Let's solve this math problem step by step. First, I will understand the problem, then break it down into smaller, manageable parts, and finally arrive at the correct answer."
  config.extractor_text:
    value: "Therefore, the final answer (use exactly this format: $NUMBER$, where NUMBER is a positive or negative integer) is $"

  config.n_steps:
    value: 10
  
  config.test_steps:
    values: [1]
  
  config.batch_size:
    values: [8]
  
  config.topk:
    distribution: int_uniform
    min: 5
    max: 25
  
  config.topq:
    distribution: int_uniform
    min: 3
    max: 12
  
  config.control_weight:
    values: [0.1, 0.5, 1.0, 2.0]
  
  config.target_weight:
    values: [0.1, 0.5, 1.0, 2.0]

# Environment variable configurations (informational - set these in your sweep agent)
# export OMP_NUM_THREADS=$(nproc)
# export TOKENIZERS_PARALLELISM=false

