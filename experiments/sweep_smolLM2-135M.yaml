# Weights & Biases Sweep Configuration for SDLM SmolLM2-135M Optimization
# Based on transfer_sdlm_smollm2_135m_instruct_gpu configuration for GSM8K dataset

program: main.py
method: bayes  # Bayesian optimization for efficient hyperparameter exploration
metric:
  name: train/loss 
  goal: mainimize

# Early termination configuration for efficient resource utilization
#early_terminate:
#  type: hyperband
#  min_iter: 15
#  eta: 3
#  s: 2

# Fixed parameters (derived from SmolLM2-135M bash script configuration)
parameters:
  # Core configuration and model setup
  config:
    value: "./configs/transfer_sdlm_smollm2_135m_instruct_gpu.py"
  
  # Weights & Biases integration
  config.use_wandb:
    value: true
  config.project:
    value: "GreaTer-SDLM"
  
  # Data configuration
  config.train_data:
    value: "../data/grade_school_math/train.jsonl"
  config.test_data:
    value: "../data/grade_school_math/test.jsonl"
  config.result_prefix:
    value: "results/sdlm_smollm2_135m_gpu_gsm8k"
  
  # Training behavior and constraints
  config.stop_on_success:
    value: true
  config.allow_non_ascii:
    value: false
  config.num_train_models:
    value: 1
  config.n_train_data:
    value: 100
  config.n_test_data:
    value: 20
  config.anneal:
    value: true
  
  # Model-specific SDLM parameters
  config.sdlm_variable_kwargs.init_strategy:
    values: ['fluency', 'random']
  config.sdlm_variable_kwargs.learnable_temperature:
    values: [false, true]
  
  # Token generation constraints
  config.acc_grad_n_examples:
    value: 2
  config.gradient_comp_batch_size:
    values: [1, 2]
  config.update_solution_max_new_tokens:
    value: 256
  config.max_new_tokens_answer:
    value: 8
  
  # Prompt engineering configuration
  config.control_init:
    value: "Let's solve this math problem step by step. First, I will understand the problem, then break it down into smaller, manageable parts, and finally arrive at the correct answer."
  config.extractor_text:
    value: "Therefore, the final answer (use exactly this format: $NUMBER$, where NUMBER is a positive or negative integer) is $"

  # Primary hyperparameters for optimization
  config.sdlm_variable_kwargs.learning_rate:
    values: [0.1, 0.1, 0.001]
  
  config.sdlm_variable_kwargs.temperature:
    values: [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
  
  config.n_steps:
    value: 10
  
  config.test_steps:
    values: [1]
  
  config.batch_size:
    values: [8]
  
  config.temp:
    distribution: uniform
    min: 0.1
    max: 1.0
  
  config.topk:
    distribution: int_uniform
    min: 5
    max: 25
  
  config.topq:
    distribution: int_uniform
    min: 3
    max: 12
  
  config.control_weight:
    distribution: uniform
    min: 0.1
    max: 0.8
  
  config.target_weight:
    distribution: uniform
    min: 0.5
    max: 2.0

# Environment variable configurations (informational - set these in your sweep agent)
# export OMP_NUM_THREADS=$(nproc)
# export TOKENIZERS_PARALLELISM=false

---
# Alternative configuration for focused parameter exploration
# Uncomment and modify for different experimental strategies

# method: grid
# parameters:
#   config.sdlm_variable_kwargs.learning_rate:
#     values: [0.005, 0.01, 0.02, 0.05]
#   config.sdlm_variable_kwargs.temperature:
#     values: [0.7, 1.0, 1.3, 1.6]
#   config.batch_size:
#     values: [8, 16, 32]
#   config.control_weight:
#     values: [0.2, 0.4, 0.6]
#   config.target_weight:
#     values: [0.8, 1.0, 1.2]

---
# Aggressive random search configuration for comprehensive exploration
# method: random
# parameters:
#   config.sdlm_variable_kwargs.learning_rate:
#     distribution: log_uniform_values
#     min: 0.0005
#     max: 0.2
#   config.sdlm_variable_kwargs.temperature:
#     distribution: uniform
#     min: 0.3
#     max: 3.0
#   config.n_steps:
#     distribution: int_uniform
#     min: 15
#     max: 150
#   config.batch_size:
#     values: [4, 8, 12, 16, 24, 32, 48]
#   config.temp:
#     distribution: uniform
#     min: 0.1
#     max: 1.5
#   config.topk:
#     distribution: int_uniform
#     min: 3
#     max: 40
#   config.topq:
#     distribution: int_uniform
#     min: 2
#     max: 20
#   config.control_weight:
#     distribution: log_uniform_values
#     min: 0.05
#     max: 1.5
#   config.target_weight:
#     distribution: log_uniform_values
#     min: 0.2
#     max: 4.0
