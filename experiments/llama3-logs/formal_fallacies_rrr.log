allow_non_ascii: false
anneal: true
attack: gcg
batch_size: 64
control_init: ' proper logical reasoning and think step by step. Finally give the
  actual correct answer.'
control_weight: 0.2
conversation_templates:
- llama-3
- llama-3
data_offset: 0
devices:
- cuda:0
- cuda:1
extractor_text: 'Therefore, the final answer (use exact format: ''$ valid'' or ''$
  invalid'') is $ '
filter_cand: true
gbda_deterministic: true
incr_control: false
logfile: ''
lr: 0.01
model_kwargs:
- low_cpu_mem_usage: true
  use_cache: false
- low_cpu_mem_usage: true
  use_cache: false
model_paths:
- meta-llama/Meta-Llama-3-8B-Instruct
- meta-llama/Meta-Llama-3-8B-Instruct
n_steps: 125
n_test_data: 50
n_train_data: 50
num_train_models: 1
progressive_goals: true
progressive_models: false
result_prefix: results/transfer_llama3_formal_fallacies_ffrf.json
stop_on_success: false
target_weight: 1.0
temp: 1
test_data: ../data/BBH/formal_fallacies.json
test_steps: 500
tokenizer_kwargs:
- add_bos_token: false
  pad_token: <|end_of_text|>
  use_fast: false
- add_bos_token: false
  pad_token: <|end_of_text|>
  use_fast: false
tokenizer_paths:
- meta-llama/Meta-Llama-3-8B-Instruct
- meta-llama/Meta-Llama-3-8B-Instruct
topk: 40
train_data: ../data/BBH/formal_fallacies.json
transfer: true
verbose: true

Loaded 50 train goals
Loaded 50 test goals
Loaded 2 tokenizers
Loaded 2 conversation templates
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:03,  1.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.66it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.68it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]Initialized Workers...

Started worker 1592368 for model meta-llama/Meta-Llama-3-8B-Instruct
Process Process-1:
Traceback (most recent call last):
  File "/home/nlp/sfd5525/anaconda3/envs/torch_cloned/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/nlp/sfd5525/anaconda3/envs/torch_cloned/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "../llm_opt/base/attack_manager.py", line 2466, in run
    results.put(ob.grad(*args, **kwargs))
  File "../llm_opt/base/attack_manager.py", line 1019, in grad
    return sum([prompt.grad(model, current_pos, valid_tokens) for prompt in self._prompts])
  File "../llm_opt/base/attack_manager.py", line 1019, in <listcomp>
    return sum([prompt.grad(model, current_pos, valid_tokens) for prompt in self._prompts])
  File "../llm_opt/gcg/gcg_attack.py", line 122, in grad
    return token_gradients(
  File "../llm_opt/gcg/gcg_attack.py", line 109, in token_gradients
    loss.backward()
  File "/home/nlp/sfd5525/anaconda3/envs/torch_cloned/lib/python3.8/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/nlp/sfd5525/anaconda3/envs/torch_cloned/lib/python3.8/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/nlp/sfd5525/anaconda3/envs/torch_cloned/lib/python3.8/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 
[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/nlp/sfd5525/anaconda3/envs/torch_cloned/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 33 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
